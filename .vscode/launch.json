{
    // 使用 IntelliSense 了解相关属性。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "Debug: llama_get_activations (NQ)",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/llama_get_activations.py",
            "args": [
                "--model_name", "llama3_8B_instruct",
                "--dataset_name", "nq",
                "--nq_jsonl", "RAG/new_dataset.jsonl",
                "--use_chat_template"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TRANSFORMERS_VERBOSITY": "info",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256"
            }
        },
        {
            "name": "Debug: llama_get_activations (NQ, 2x3090)",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/llama_get_activations.py",
            "args": [
                "--model_name", "llama3_8B_instruct",
                "--dataset_name", "nq",
                "--nq_jsonl", "RAG/new_dataset.jsonl",
                "--use_chat_template"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1",
                "TRANSFORMERS_VERBOSITY": "info",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256"
            }
        },
        {
            "name": "Debug: nq_train_save_probes (NQ, 5-fold)",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/nq_train_save_probes.py",
            "args": [
                "--model_name", "llama3_8B_instruct",
                "--top_k", "48",
                "--seed", "2025",
                "--num_fold", "5"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1",
                "TRANSFORMERS_VERBOSITY": "info"
            }
        },
        {
            "name": "Debug: nq_con_rag (CoN two-stage, Llama-2-7b)",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/nq_con_rag.py",
            "python": "/root/.conda/envs/iti/bin/python",
            "args": [
                "--model_name", "llama2_chat_7B",
                "--dataset_path", "RAG/data/NQ/test_noise_test_noise4.jsonl",
                "--use_chat_template",
                "--max_docs", "5",
                "--sample_size", "10",
                "--max_new_tokens", "256",
                "--results_root", "RAG/results/llama-2-chat-7b-nq-user/CoN_300"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TRANSFORMERS_VERBOSITY": "info",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256"
            }
        },
        {
            
            "name": "Debug: nq_naive_llm (no retrieval)",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/nq_naive_llm.py",
            "python": "/root/.conda/envs/iti/bin/python",
            "args": [
                "--model_name", "llama2_chat_7B",
                "--dataset_path", "RAG/data/NQ/test_noise_test_noise4.jsonl",
                "--use_chat_template",
                "--sample_size", "10",
                "--max_new_tokens", "256",
                "--results_root", "RAG/results/llama-3-8b-instruct-nq-user/naive-llm"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TRANSFORMERS_VERBOSITY": "info",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256"
            }
        },
        {
            "name": "Debug: 3_extract_remaining_testset_popqa",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}",
            "program": "${workspaceFolder}/RAG/utils/3_extract_remaining_testset_popqa.py",
            "python": "/root/.conda/envs/iti/bin/python",
            "args": [
                "--input", "RAG/data/PopQA/PopQA_processed.jsonl",
                "--val-output", "RAG/data/PopQA/val_noise_test.jsonl",
                "--test-output", "RAG/data/PopQA/test_noise_test.jsonl",
                "--train-num", "0",
                "--val-num", "0",
                "--test-num", "1190",
                "--method", "random",
                "--seed", "2025",
                "--noise-levels", "0,1,2,3,4,5"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TRANSFORMERS_VERBOSITY": "info",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256"
            },
        }
        
    ]
}
