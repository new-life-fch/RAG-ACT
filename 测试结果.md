python RAG/llama_get_activations.py --model_name llama2_chat_7B --dataset_name nq --nq_jsonl RAG/data/train.jsonl --use_chat_template

python RAG/nq_train_save_probes.py --model_name llama2_chat_7B --top_k 24 --seed 2025 --num_fold 5 --cv_final_train full

python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 15 --top_heads_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_24_folds_5_top_heads.pkl --probes_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_24_folds_5_probes.pkl --val_accs_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_24_folds_5_val_accs.npy --tuning_headwise_path features/llama2_chat_7B_nq_head_wise.npy --sample_size 30 --sample_seed 2025 --max_new_tokens 256

top-k alpha EM(RAG) F1(RAG) EM(interventions) F1(interventions)
24 15 0.43333333333333335 0.5474490244055461 0.1 0.29398768513858026
24 10 0.43333333333333335 0.5474490244055461  0.2 0.36727801607664307
24 5 0.43333333333333335 0.5474490244055461 0.36666666666666664 0.48713926009578185



python RAG/llama_get_activations.py --model_name llama2_chat_7B --dataset_name nq --nq_jsonl RAG/data/train.jsonl --use_chat_template

python RAG/nq_train_save_probes.py --model_name llama2_chat_7B --top_k 12 --seed 2025 --num_fold 5 --cv_final_train full

python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 15 --top_heads_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_12_folds_5_top_heads.pkl --probes_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_12_folds_5_probes.pkl --val_accs_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_12_folds_5_val_accs.npy --tuning_headwise_path features/llama2_chat_7B_nq_head_wise.npy --sample_size 30 --sample_seed 2025 --max_new_tokens 256

top-k alpha EM(RAG) F1(RAG) EM(interventions) F1(interventions)
12 15 0.43333333333333335 0.5474490244055461 0.0 0.1760108695968442
12 10 0.43333333333333335 0.5474490244055461  0.06666666666666667 0.2644052057782035
12 5 0.43333333333333335 0.5474490244055461 0.26666666666666666 0.4005784898384845


python RAG/llama_get_activations.py --model_name llama2_chat_7B --dataset_name nq --nq_jsonl RAG/data/train.jsonl --use_chat_template

python RAG/nq_train_save_probes.py --model_name llama2_chat_7B --top_k 36 --seed 2025 --num_fold 5 --cv_final_train full

python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 15 --top_heads_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_36_folds_5_top_heads.pkl --probes_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_36_folds_5_probes.pkl --val_accs_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_36_folds_5_val_accs.npy --tuning_headwise_path features/llama2_chat_7B_nq_head_wise.npy --sample_size 30 --sample_seed 2025 --max_new_tokens 256

top-k alpha EM(RAG) F1(RAG) EM(interventions) F1(interventions)
36 15 0.43333333333333335 0.5474490244055461 0.06666666666666667 0.26693793000668203
36 10 0.43333333333333335 0.5474490244055461  0.2 0.36474194506231117
36 5 0.43333333333333335 0.5474490244055461 0.3333333333333333 0.4536852460275624
36 20 0.43333333333333335 0.5474490244055461 0.0 0.14118668626408565


python RAG/llama_get_activations.py --model_name llama2_chat_7B --dataset_name nq --nq_jsonl RAG/data/train.jsonl --use_chat_template

python RAG/nq_train_save_probes.py --model_name llama2_chat_7B --top_k 36 --seed 2025 --num_fold 5 --cv_final_train full

python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 15 --top_heads_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_64_folds_5_top_heads.pkl --probes_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_64_folds_5_probes.pkl --val_accs_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_64_folds_5_val_accs.npy --tuning_headwise_path features/llama2_chat_7B_nq_head_wise.npy --sample_size 50 --sample_seed 2025 --max_new_tokens 256

top-k alpha EM(RAG) F1(RAG) EM(interventions) F1(interventions)
36 15 0.43333333333333335 0.5474490244055461 0.06666666666666667 0.26693793000668203
36 10 0.43333333333333335 0.5474490244055461  0.2 0.36474194506231117
36 5 0.43333333333333335 0.5474490244055461 0.3333333333333333 0.4536852460275624
36 20 0.43333333333333335 0.5474490244055461 0.0 0.14118668626408565



python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 0.4 --top_heads_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_1024_folds_5_top_heads.pkl --probes_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_1024_folds_5_probes.pkl --val_accs_path results_dump/probes/llama2_chat_7B_nq_seed_2025_top_1024_folds_5_val_accs.npy --tuning_headwise_path features/llama2_chat_7B_nq_head_wise.npy --sample_size 50 --sample_seed 2025 --max_new_tokens 256


(iti) root@p-f9d35f173ad8-ackcs-00gjfi8o:~/shared-nvme/RAG-llm# python RAG/nq_generate_with_interventions.py --model_name llama2_chat_7B --dataset_path RAG/data/test.jsonl --use_chat_template --alpha 5 --top_heads_path results_dump/probes/llama2_chat_7
B_nq_seed_2025_top_1024_folds_5_top_heads.pkl --probes_path results_dump/probes/llam
a2_chat_7B_nq_seed_2025_top_1024_folds_5_probes.pkl --val_accs_path results_dump/pro
bes/llama2_chat_7B_nq_seed_2025_top_1024_folds_5_val_accs.npy --tuning_headwise_path
 features/llama2_chat_7B_nq_head_wise.npy --sample_size 50 --sample_seed 2025 --max_
new_tokens 256
nq_generate: 100%|██████████████████████████████████| 50/50 [01:11<00:00,  1.43s/it]
Baseline Summary: {'EM': 0.4, 'F1': 0.5426837003576135, 'alpha': 0.0, 'model_name': 'llama2_chat_7B', 'intervention': 'none'}
Intervention Summary: {'EM': 0.52, 'F1': 0.6724110275689223, 'alpha': 5.0, 'model_name': 'llama2_chat_7B', 'intervention': 'probe_top48_with_factor'}

Baseline Summary: {'EM': 0.4, 'F1': 0.5426837003576135, 'alpha': 0.0, 'model_name': 'llama2_chat_7B', 'intervention': 'none'}
Intervention Summary: {'EM': 0.52, 'F1': 0.6641904761904762, 'alpha': 6.0, 'model_name': 'llama2_chat_7B', 'intervention': 'probe_top48_with_factor'}
Baseline Summary: {'EM': 0.4, 'F1': 0.5426837003576135, 'alpha': 0.0, 'model_name': 'llama2_chat_7B', 'intervention': 'none'}
Intervention Summary: {'EM': 0.52, 'F1': 0.6608571428571429, 'alpha': 5.5, 'model_name': 'llama2_chat_7B', 'intervention': 'probe_top48_with_factor'}